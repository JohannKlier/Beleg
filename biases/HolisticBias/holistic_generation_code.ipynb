{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# noun-phrase table (≈18 k rows)\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"fairnlp/holistic-bias\", \"sentences\",\n",
    "                       split=\"test\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_to_keep = [\n",
    "    \"gender_and_sex\",\n",
    "    \"nationality\",\n",
    "    \"religion\",\n",
    "    \"sexual_orientation\",\n",
    "    \"cultural\"\n",
    "]\n",
    "\n",
    "mask = (\n",
    "    ds[\"axis\"].isin(axes_to_keep)                                # ① axis filter                                  # ② noun present\n",
    "    & ds.apply(                                                  # ③ template ends with {noun_phrase}.\n",
    "        lambda r: r[\"template\"].rstrip().endswith(\"{noun_phrase}.\"),\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "df_filtered = ds[mask].reset_index(drop=True)\n",
    "print(df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    # \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, math\n",
    "\n",
    "MODEL_NAME = MODELS[0]    # pick your model\n",
    "tokenizer   = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch, torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ───────────────────────── helpers ─────────────────────────\n",
    "def rfind_subseq(hay, needle):\n",
    "    for i in range(len(hay) - len(needle), -1, -1):\n",
    "        if hay[i : i + len(needle)] == needle:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def continuation_stats(cut_sentence, continuation, full_sentence):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Finish the following sentence:\\n\\n{cut_sentence}\"},\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": full_sentence}\n",
    "    ]\n",
    "    ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids).logits\n",
    "    logp = F.log_softmax(logits.float(), dim=-1)[0]\n",
    "\n",
    "    assistant_ids = tokenizer(full_sentence, add_special_tokens=False).input_ids\n",
    "    a_start = rfind_subseq(ids[0].tolist(), assistant_ids)\n",
    "    if a_start < 0:\n",
    "        raise ValueError(\"assistant span not found\")\n",
    "\n",
    "    prefix_ids = tokenizer(cut_sentence, add_special_tokens=False).input_ids\n",
    "    cont_ids   = assistant_ids[len(prefix_ids):]\n",
    "\n",
    "    c_start = a_start + len(prefix_ids)\n",
    "    ctx_rows   = logp[c_start - 1 : c_start - 1 + len(cont_ids)]\n",
    "    idx        = torch.tensor(cont_ids, device=logp.device).unsqueeze(-1)\n",
    "    token_logp = ctx_rows.gather(-1, idx).squeeze(-1)\n",
    "\n",
    "    token_logps = token_logp.tolist()          # 1-D list[float]\n",
    "    total_logp  = sum(token_logps)\n",
    "    n_tokens    = len(token_logps)\n",
    "    avg_logp    = total_logp / n_tokens\n",
    "    perplexity  = math.exp(-avg_logp)\n",
    "\n",
    "    return {\n",
    "        \"token_logps\": token_logps,\n",
    "        \"logP_continuation\": total_logp,\n",
    "        \"n_cont_tokens\": n_tokens,\n",
    "        \"avg_logP_per_token\": avg_logp,\n",
    "        \"perplexity\": perplexity,\n",
    "    }\n",
    "\n",
    "\n",
    "# ───────────────────────── main loop ───────────────────────\n",
    "records = []\n",
    "for _, row in tqdm(df_filtered.iterrows(),\n",
    "                   total=len(df_filtered),\n",
    "                   desc=\"scoring\"):\n",
    "\n",
    "    template    = row[\"template\"]\n",
    "    noun_phrase = row[\"noun_phrase\"]\n",
    "\n",
    "    pre, post = template.split(\"{noun_phrase}\", 1)\n",
    "    cut_sent      = pre\n",
    "    continuation  = \" \" + noun_phrase + post\n",
    "    full_sent     = cut_sent + continuation\n",
    "\n",
    "    stats = continuation_stats(cut_sent, continuation, full_sent)\n",
    "\n",
    "    rec = row.to_dict()\n",
    "    rec.update(stats)                 # merge the new metrics\n",
    "    records.append(rec)\n",
    "\n",
    "df_with_logp = pd.DataFrame(records)\n",
    "\n",
    "# optional: persist to disk\n",
    "# df_with_logp.to_parquet(\"filtered_with_logp.parquet\")   # keeps list columns intact\n",
    "# df_with_logp.to_csv(\"filtered_with_logp.csv\", index=False)  # lists will beb JSON-ish\n",
    "\n",
    "df_with_logp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_logp.to_csv(\"test_qwen_long.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
