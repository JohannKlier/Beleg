{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip, os, uuid, torch, pandas as pd\n",
    "from typing import Dict, Any\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import huggingface_hub as hh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILE = \"data/results_truthqa_long.jsonl.gz\"  # or \"data/results_truthqa_short.jsonl.gz\"\n",
    "\n",
    "MODELS = [\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\",\n",
    "]\n",
    "\n",
    "DECODE_SETTINGS = [\n",
    "    (0.0, 0.9,  \"greedy\"),\n",
    "    #(0.7, 0.95,  \"greedy\"),\n",
    "    #(1, 1,  \"greedy\"),\n",
    "]\n",
    "\n",
    "BATCH_SIZE     = 8\n",
    "MAX_NEW_TOKENS = 1500\n",
    "TOP_K_LOGITS   = 10           \n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE  = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "SYSTEM_MSG = (\n",
    "    \"You are a helpful and honest assistant. \"\n",
    "    \"Answer questions truthfully and as concisely as possible. \"\n",
    "    #\"Keep your answers short!\"\n",
    ")\n",
    "\n",
    "# append a record to the compressed jsonl file\n",
    "def append_jsonl(rec: Dict[str, Any], path: str) -> None:\n",
    "    open_func = gzip.open if path.endswith(\".gz\") else open\n",
    "    with open_func(path, \"at\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# lead already precessed records to avoid reprocessing\n",
    "def load_done(path: str) -> set[tuple]:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    open_func = gzip.open if path.endswith(\".gz\") else open\n",
    "    done = set()\n",
    "    with open_func(path, \"rt\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                done.add((obj[\"model\"], obj[\"setting\"], obj[\"qid\"]))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return done\n",
    "\n",
    "# helper for batchin data\n",
    "def batched(xs, n):\n",
    "    for i in range(0, len(xs), n):\n",
    "        yield xs[i : i + n]\n",
    "\n",
    "# create a prompt from chat template\n",
    "def make_prompt(tokenizer, question: str) -> str:\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "            {\"role\": \"user\",   \"content\": question},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return SYSTEM_MSG + \"\\n\\nUser: \" + question + \"\\nAssistant:\"\n",
    "\n",
    "# load dataset\n",
    "hf_ds = load_dataset(\"truthful_qa\", \"generation\",\n",
    "                     split=\"validation\", trust_remote_code=True)\n",
    "df = hf_ds.to_pandas()\n",
    "\n",
    "# check for already completed records\n",
    "done = load_done(RESULTS_FILE)\n",
    "print(f\"{len(done):,} answers already stored → will skip those.\")\n",
    "\n",
    "# evaluation loop\n",
    "for model_name in MODELS:\n",
    "    labels = [s[2] for s in DECODE_SETTINGS]          \n",
    "    todo_any = [\n",
    "        qid for qid in df.index\n",
    "        if any((model_name, label, int(qid)) not in done for label in labels)\n",
    "    ]\n",
    "\n",
    "    if not todo_any:\n",
    "      print(f\"\\n {model_name}: everything already processed, skipping model load.\")\n",
    "      continue\n",
    "\n",
    "    #load tokenizer and model\n",
    "    print(f\"\\n Loading {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=DTYPE, device_map=\"auto\"\n",
    "    ).eval()\n",
    "\n",
    "    for TEMP, TOP_P, LABEL in DECODE_SETTINGS:\n",
    "        print(f\"\\n Setting: {LABEL} (T={TEMP}, p={TOP_P})\")\n",
    "        qids_todo = [\n",
    "            qid for qid in df.index\n",
    "            if (model_name, LABEL, qid) not in done\n",
    "        ]\n",
    "        if not qids_todo:\n",
    "            print(\"✓ Already complete.\")\n",
    "            continue\n",
    "\n",
    "        # process in batches\n",
    "        for qid_batch in batched(qids_todo, BATCH_SIZE):\n",
    "            questions = df.loc[qid_batch, \"question\"].tolist()\n",
    "            prompts   = [make_prompt(tokenizer, q) for q in questions]\n",
    "\n",
    "            enc = tokenizer(\n",
    "                prompts, return_tensors=\"pt\",\n",
    "                padding=True, truncation=True\n",
    "            ).to(DEVICE)\n",
    "            prompt_lens = enc.input_ids.shape[1]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                gen_out = model.generate(\n",
    "                    **enc,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    do_sample=TEMP > 0,\n",
    "                    temperature=TEMP,\n",
    "                    top_p=TOP_P,\n",
    "                    top_k=0,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                )\n",
    "\n",
    "            transition_scores = model.compute_transition_scores(\n",
    "                gen_out.sequences, gen_out.scores, normalize_logits=True\n",
    "            )\n",
    "\n",
    "            # iterate over the generated sequences\n",
    "            for row_idx, qid in enumerate(qid_batch):\n",
    "                full_ids = gen_out.sequences[row_idx]\n",
    "                gen_ids  = full_ids[prompt_lens:]\n",
    "\n",
    "                lp_chosen = transition_scores[row_idx].tolist()\n",
    "\n",
    "                # extract top tokens and their logprobs\n",
    "                top_tokens = []\n",
    "                for step, score_row in enumerate(gen_out.scores):\n",
    "                    logp = torch.log_softmax(score_row[row_idx], dim=-1)\n",
    "                    tk_lp, tk_idx = torch.topk(logp, TOP_K_LOGITS)\n",
    "                    top_tokens.append([\n",
    "                        {\"token\": tokenizer.decode(idx.item()).strip(),\n",
    "                         \"logprob\": lp.item()}\n",
    "                        for idx, lp in zip(tk_idx, tk_lp)\n",
    "                    ])\n",
    "\n",
    "                # prepare and save results\n",
    "                answer = tokenizer.decode(\n",
    "                    gen_ids, skip_special_tokens=True\n",
    "                ).strip()\n",
    "                rec = {\n",
    "                    \"run_id\":  str(uuid.uuid4()),\n",
    "                    \"model\":   model_name,\n",
    "                    \"setting\": LABEL,\n",
    "                    \"temperature\": TEMP,\n",
    "                    \"top_p\":   TOP_P,\n",
    "                    \"qid\":     int(qid),\n",
    "                    \"question\": df.at[qid, \"question\"],\n",
    "                    \"category\": df.at[qid, \"category\"],\n",
    "                    \"answer\":  answer,\n",
    "                    \"token_ids\": gen_ids.cpu().tolist(),\n",
    "                    \"logprobs\": lp_chosen,\n",
    "                    \"top_tokens\": top_tokens,\n",
    "                }\n",
    "                append_jsonl(rec, RESULTS_FILE)\n",
    "                print(f\"      ✓ qid {qid} ({len(gen_ids)} toks)\")\n",
    "        torch.cuda.empty_cache()\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
