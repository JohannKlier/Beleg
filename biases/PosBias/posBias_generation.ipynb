{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random, torch, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODELS = [\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\",\n",
    "]\n",
    "\n",
    "# model prompt for multiple-choice questions\n",
    "FEW_SHOT_MSG = (\n",
    "    \"You will be asked multiple-choice questions. \"\n",
    "    \"Each question has **exactly one correct answer**.\\n\"\n",
    "    \"Return **only** the letter (A, B, C or D) of the option you believe is correctâ€”\"\n",
    "    \"no explanation, no punctuation, nothing else.\\n\\n\"\n",
    "\n",
    "    \"Example 1:\\n\"\n",
    "    \"Question: What is the capital of France?\\n\"\n",
    "    \"Choices:\\n\"\n",
    "    \"A. Berlin\\nB. Madrid\\nC. Paris\\nD. Rome\\n\"\n",
    "    \"FINAL ANSWER: C\\n\\n\"\n",
    "\n",
    "    \"Example 2:\\n\"\n",
    "    \"Question: What gas do plants primarily use for photosynthesis?\\n\"\n",
    "    \"Choices:\\n\"\n",
    "    \"A. Nitrogen\\nB. Oxygen\\nC. Hydrogen\\nD. Carbon Dioxide\\n\"\n",
    "    \"FINAL ANSWER: D\\n\\n\"\n",
    ")\n",
    "\n",
    "# build prompt with chat template\n",
    "def build_chat_prompt(tok: AutoTokenizer, question: str, choices: list[str]) -> str:\n",
    "    user_msg = (\n",
    "        f\"{FEW_SHOT_MSG}\\n\\n\"\n",
    "        f\"Question: {question}\\nChoices:\\n\" +\n",
    "        \"\\n\".join(f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)) +\n",
    "        \"\\nFINAL ANSWER:\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a careful, factual assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": user_msg},\n",
    "    ]\n",
    "    return tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# identify token IDs corresponding to each answer letter\n",
    "def collect_label_token_ids(letter: str, tok: AutoTokenizer) -> list[int]:\n",
    "    ids = []\n",
    "    for prefix in (\" \", \"\\n\", \"\"):\n",
    "        variant = prefix + letter\n",
    "        if len(tok.tokenize(variant)) == 1:\n",
    "            ids.append(tok(variant, add_special_tokens=False).input_ids[0])\n",
    "    if not ids:\n",
    "        raise ValueError(f\"No single-token rendition for letter '{letter}'\")\n",
    "    return ids\n",
    "\n",
    "# load mc data\n",
    "df_truthful = load_dataset(\"EleutherAI/truthful_qa_mc\", split=\"validation\").to_pandas()\n",
    "\n",
    "records = []                        \n",
    "\n",
    "# iterate over models\n",
    "for model_name in MODELS:\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "\n",
    "    # load model and tokenizer\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    ).eval()\n",
    "\n",
    "\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    if getattr(mdl.config, \"pad_token_id\", None) is None:\n",
    "        mdl.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    \n",
    "    label_ids = {l: collect_label_token_ids(l, tok) for l in \"ABCD\"}\n",
    "\n",
    "    prompts, gold_letters, qids = [], [], []\n",
    "    for qid, row in df_truthful.iterrows():\n",
    "        question     = row[\"question\"]\n",
    "        all_choices  = row[\"choices\"]\n",
    "        correct_idx  = row[\"label\"]\n",
    "        correct_ans  = all_choices[correct_idx]\n",
    "        incorrects   = [a for i, a in enumerate(all_choices) if i != correct_idx]\n",
    "\n",
    "        # shuffle positions of correct answer \n",
    "        for pos in range(4):        \n",
    "            distractors = random.sample(incorrects, k=min(3, len(incorrects)))\n",
    "            if len(distractors) < 3:\n",
    "                distractors += [\"(None)\"] * (3 - len(distractors))\n",
    "\n",
    "            choices = distractors[:]\n",
    "            choices.insert(pos, correct_ans)\n",
    "            prompts.append(build_chat_prompt(tok, question, choices))\n",
    "            gold_letters.append(chr(65 + pos))\n",
    "            qids.append(qid)\n",
    "\n",
    "    # evaluate model reponses in batches\n",
    "    BATCH = 8\n",
    "    for start in tqdm(range(0, len(prompts), BATCH), desc=model_name):\n",
    "        batch_prompts  = prompts[start:start+BATCH]\n",
    "        batch_correct  = gold_letters[start:start+BATCH]\n",
    "        batch_qid      = qids[start:start+BATCH]\n",
    "\n",
    "        enc = tok(batch_prompts, return_tensors=\"pt\",\n",
    "                  padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = mdl(**enc).logits        \n",
    "\n",
    "        # get logits for final token prediction\n",
    "        T = enc[\"attention_mask\"].size(1)\n",
    "        last_idx = (T - 1) - enc[\"attention_mask\"].flip(-1).argmax(-1)\n",
    "\n",
    "        batch_logits = logits[torch.arange(logits.size(0), device=logits.device), last_idx]\n",
    "\n",
    "        # record predictions and log probabilities\n",
    "        for j in range(len(batch_prompts)):\n",
    "            lp = torch.nn.functional.log_softmax(batch_logits[j], dim=0)\n",
    "\n",
    "            option_lp = {l: torch.logsumexp(lp[ids], dim=0).item()\n",
    "                         for l, ids in label_ids.items()}\n",
    "            pred = max(option_lp, key=option_lp.get)\n",
    "\n",
    "            records.append({\n",
    "                \"qid\":   batch_qid[j],\n",
    "                \"model\": model_name,\n",
    "                \"gold\":  batch_correct[j],\n",
    "                \"pred\":  pred,\n",
    "                **option_lp\n",
    "            })\n",
    "\n",
    "    del mdl; torch.cuda.empty_cache()\n",
    "\n",
    "# save reults\n",
    "df_eval = pd.DataFrame(records)\n",
    "df_eval[\"correct\"] = df_eval[\"gold\"] == df_eval[\"pred\"]\n",
    "df_eval.to_csv(\"pos_bias.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
